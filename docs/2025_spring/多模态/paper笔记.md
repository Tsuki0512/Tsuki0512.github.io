## 0-1 什么是具备多模态理解能力的MLLM

### 0-1-1 Visual Instruction Turing

- [论文链接](https://arxiv.org/abs/2304.08485)

- 参考链接：[1](https://blog.csdn.net/qq_58400270/article/details/135073408) [2](https://zhuanlan.zhihu.com/p/647782091)

#### 1 前置知识

![image-20250303105247532](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303105247532.png)

指令微调和提示微调的目的都是去挖掘语言模型本身具备的知识。不同的是，Prompt 是激发语言模型的**补全能力**，例如根据上半句生成下半句，或是完形填空等。Instruct 是激发语言模型的**理解能力**，它通过给出更明显的指令，让模型去做出正确的行动。**指令微调的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示微调都是针对一个任务的。泛化能力不如指示学习**。

#### 2 具体实现

##### 2.1 视觉指令数据集

用于指令微调的指示学习阶段。

![image-20250303111943784](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303111943784.png)

我们的输入并不是图片，而是描述图片的文本信息：captions和boxes。

训练过程中，模型通过图片描述和三类指令给出回答。

##### 2.2 LLaVa架构

![image-20250303111756590](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303111756590.png)

用一个简单的例子，比如你给系统输入一张狗的图片，并问它：“这是什么动物？”
LLaVA 处理这个问题的流程如下：

1. **图片输入 (`X_v`)**
      - 你上传一张狗的照片 (`X_v`)。
      - 视觉编码器（ViT-L/14）处理这张图片，提取重要特征，得到 `Z_v`（相当于“图片的数值版”）。
2. **特征转换 (`Z_v` → `H_v`)**
      - 由于 LLM 只能理解文本，所以需要把 `Z_v` 变成类似单词的格式。
      - 通过**投影矩阵 (`W`)**，把 `Z_v` 转换成**视觉标记 (`H_v`)**，让 LLM 能够理解。
3. **语言模型处理 (`H_v` + `H_q`)**
      - 你的问题 `"这是什么动物？"` (`X_q`) 也被语言模型处理。
      - 语言模型结合问题 (`H_q`) 和视觉信息 (`H_v`)，然后生成回答 (`X_a`)。
4. **输出 (`X_a`)**
      - 模型输出 `"这是一只狗"`，完成任务。

##### 2.3 训练过程

![image-20250303135208878](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135208878.png)

整体训练的输入输出模式为：

![image-20250303135334852](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135334852.png)

- 绿色的部分用于计算loss。*q: 为什么所有<STOP>token都要被计算？*

- 指令微调过程的输出计算采用自回归函数，也就是给定图像特征和指令后输出为a的条件概率为所有token在图像特征和之前所有token的条件下输出该token的条件概率相乘；对于长度为$L$的对话数据序列，计算可能的回答概率如下：
   ![image-20250303135721366](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135721366.png)

- $X_{\text{system-message}}$为系统要执行的任务：

  ```c++
  A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human’s questions. and <STOP> = ###
  ```

###### 2.3.1 第一阶段：特征对齐预训练

在2.2的架构中，我们看到语言模型会结合`H_v`和`H_q`给出回答，所以我们在这一步需要做的就是对齐两者的特征。

利用 [图像+人工标记的caption] 数据集，冻结视觉编码器和LLM的参数，仅训练线性层$\text{Projection}W$的参数，在构建输入$X_{instruction}$时，对于每一张图像$X_v$，随机抽取一个问题$X_q$ ，这个问题其实就是让模型（可以理解为助手）简单描述图像的语言指令。真实的预测答案$X_a$就是原来图像所配的文字描述（也就是原始的图像说明文字）。

而我们训练的过程就是调整线性层参数使得$X_a$的$p$最大化。

###### 2.3.2 第二阶段：端到端的微调

始终保持视觉编码器的权重冻结（即不更新视觉编码器的权重），继续更新投影层的预训练权重以及 LLaVA 中的大语言模型（LLM）的权重。

在这里我们考虑两个特定的用例场景：

1. chatbot: 在聊天机器人输出的三种类型回复中，对话类型是多轮的，而另外两种回复类型是单轮的。在训练过程中，对这三种回复类型进行均匀采样。
2. Science QA: 将数据组织成单轮对话形式，模型需要完成两项任务，一是以自然语言的形式提供推理过程，二是在多个选项中选择答案，训练中我们把问题和上下文作为$X_{instruction}$，把推理过程和答案作为$X_a$。

### 0-1-2 MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models

- [论文链接](https://arxiv.org/abs/2304.10592)

- 参考链接：[1](https://jackcrum.blog.csdn.net/article/details/131258219?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-131258219-blog-130386728.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-131258219-blog-130386728.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=5) [2](https://blog.csdn.net/m0_52911108/article/details/142914183)

#### 1 模型结构

MiniGPT-4处理视觉和语言任务的流程：

![image-20250303152941564](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303152941564.png)

- 视觉部分
     1. **输入图像**：右下角的火烈鸟logo是输入的视觉信息。
     2. **视觉编码器**：
        - ViT（Vision Transformer）是一种用于计算机视觉任务的 Transformer 架构，它会对输入的火烈鸟 logo 图像进行特征提取，将图像信息转化为特征向量。
        - Q - Former 则可能是用于处理这些特征向量，以便后续与语言模型进行交互。
- **线性层（投影层）**</br>
   中间的橙色方框 “Linear Layer” 是一个线性层，也可理解为投影层。它的作用是将视觉编码器提取的特征向量投影到与语言模型相匹配的特征空间中，使得视觉特征能够被语言模型理解和处理，实现视觉信息与语言信息在特征层面的**融合**。</br>
   *q: LLaVA是同时训练调整视觉特征和指令特征在线性层的投影参数，mini-GPT4是指训练调整视觉特征在线性层的投影参数使其和输入的语言问题对齐？*
- 语言模型部分
     1. **Vicuna**：是一个经过训练的大语言模型，在 MiniGPT - 4 中它被冻结，参数在训练过程中不更新。
     2. **输入与输出**：图中左侧 “Human:” 代表人类输入的问题，例如 “ What do you think of this logo design? ”。经过线性层处理后的视觉特征与语言问题一起输入到 Vicuna 模型中，然后 Vicuna 模型生成回答（图中右侧 “Assistant:” ），图中给出的回答示例是一段对火烈鸟 logo 设计的评价，描述了 logo 设计简约、易识别，使用火烈鸟作为标志增添了趣味性等特点。

mini-GPT4训练的是线性层的参数，使得视觉特征和语言信息在特征层面融合。

基于该模型的主要发现：

- 该模型具有高级视觉语言能力
- 仅训练线性层即可对齐视觉编码器和LLM的特征，且训练开销不大 *q: 这里的特征对齐具体是什么含义*
- 采用image caption对模型训练效果并不好，提供小而细节的图像描述可以更好地解决这个问题

#### 2 训练过程

##### 2.1 第一阶段预训练

通过大量对齐的图片-文本对去训练模型整体的视觉-语言能力，将线性层输出作为LLM的软提示输入，目的是让LLM输出正确的回答。这个过程只训练线性层参数。

训练过程中的问题：会生成不连贯的语言输出，例如单词或句子重复、句子碎片化，或者内容不相关。为了解决这个问题，他们为视觉语言领域策划了一个高质量的对齐数据集。

数据集制作过程：

1. **初始对齐的图像-文本生成**
   使用从第一阶段预训练得到的模型来生成输入图像的全面描述，LLM输入为：
   ![image-20250304162914505](./paper%E7%AC%94%E8%AE%B0.assets/image-20250304162914505.png)
   `<ImageFeature>`是线性层的输出（视觉特征）。
   之后检查生成的句子是否超过 80 个词元，如果没有超过，添加一个额外的提示，`###Human: Continue ###Assistant:`，促使模型继续生成内容。通过将两个步骤的输出连接起来创建出更全面的图像描述。
2. **数据后处理**
   上述自动生成的图像描述包含有噪声或不连贯的内容，例如单词或句子的重复、句子支离破碎，或者出现不相关的信息。为了解决这些问题，我们使用 ChatGPT，通过以下提示来修正这些描述：
   ![image-20250304163341261](./paper%E7%AC%94%E8%AE%B0.assets/image-20250304163341261.png)
3. 人工验证每一条图像描述的正确性，以确保其高质量：
      - 找出了一些频繁出现的错误，编写硬编码规则来自动过滤掉这些错误；
      - 人工优化生成的图像说明文字，删除那些 ChatGPT 未能检测到的冗余单词或句子。

我们将处理完毕的数据集用于第二阶段的微调训练。

##### 2.2 第二阶段微调

通过第一阶段制作的数据集，从指令集中随机抽取指令作为`<Instruction>`，在LLM中输入：

![image-20250304163648618](./paper%E7%AC%94%E8%AE%B0.assets/image-20250304163648618.png)

进行指令微调训练，对于这个特定的文本 - 图像提示，我们并不计算回归损失。这使得模型能够生成更自然可靠的语言输出，且该微调过程效率较高。

*q: 为什么我们在这个过程中不计算回归损失，那我如何达到这个阶段的训练目的*

*q: 第一阶段得到的是文本-图像数据集，但是模型输入不是图像+语言吗，这个文本在这里起到什么作用*

*a: 作为训练的输入输出对的输出内容，因为指令集里面的指令都是换着不同方式表达同一个意思（描述图片details）*

## 0-2 常见的图像生成范式

### 0-2-1 High-Resolution Image Synthesis with Latent Diffusion Models

---

#### 前置1：Diffusion Model

学习链接：[1](https://www.bilibili.com/video/BV1xih7ecEMb/?buvid=Y24FF22823969EE34A5598A5D4F4695016E2&from_spmid=search.search-result.0.0&is_story_h5=false&mid=dhneVveoAEYYORADGVP2pA%3D%3D&plat_id=116&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=EF9298D6-031B-4816-AF2D-B42CAEAF07E7&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1741161005&unique_k=LyXYQ8H&up_id=527467660&vd_source=fef878b8598786a5cbf02ab622b4d684) [2](https://huggingface.co/learn/diffusion-course/unit1/1)

**1. 什么是DM？**

给定前向过程，每一次往前一张图片加噪点：

![image-20250305165621261](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305165621261.png)

加噪的具体过程是每次加一个0-1高斯分布的噪点：

![image-20250305170026101](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305170026101.png)



而我们可以考虑其反向过程：

![image-20250305165739898](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305165739898.png)

就是逐步去噪生成目标图片的过程，我们训练出一个可以预测噪声的CNN，把原始的照片减去预测出来的噪声得到$x_{t-1}$：

![image-20250305170309616](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305170309616.png)

每一步使用的模型是同一个模型，但是我们会输入$t$作为当前步骤/时刻的信息。

**2. 具体过程**

*一堆数学推导，两年前学的概率论忘完了，先放着吧*

前向加噪，通过公式推导我们可以得出这个过程中每一步都是符合高斯分布的：

![image-20250305172334527](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305172334527.png)

我们也可以通过公式推导把逐步加噪变成单步加噪：

![image-20250305172655087](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305172655087.png)

但是反向过程是不可能的，我们需要神经网络拟合，通过一个高斯分布去拟合另一个高斯分布：

![image-20250305210215469](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305210215469.png)

拟合的方式是最大似然估计，这里的数学推导真的是天书，没学会，不记了（和整个模型的使用关系不大）

后面也是天书。

**3. 训练过程**

加噪+预测噪声，采用$L2$评估损失

![image-20250305211616989](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305211616989.png)

**4. 采样过程**

逐步去噪。

![image-20250305211731727](./paper%E7%AC%94%E8%AE%B0.assets/image-20250305211731727.png)

最后一步和前面的区别是不会加扰动项了。

---

#### 前置2：Transformer

学习链接：[1](https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.337.search-card.all.click&vd_source=fef878b8598786a5cbf02ab622b4d684) *3b1b yyds*

首先这些token都是基于[word embedding](https://tsuki0512.github.io/2025_spring/nlp/WordEmbedding/)的，一个token代表了一个高维空间的向量。

核心目标：对单个token的语义增加上下文的影响（一般是上文，因为预测是基于next token方法的，下文不参与预测影响）。

实现方式：

- 单头注意力机制：我们通过$Q·K$点积判定两个token之间的相关性，再将$Q_i·K_j(i\leq j)$置为$-\infin$消除下文对之前的token的影响，使用$softmax$归一化之后在原始的token所对应的向量的基础上加上归一化的参数乘对应的token的向量$v$和$V$矩阵的乘积形成新的含上下文语义的向量。在这里这个$V$矩阵这样表示/实现：
  ![image-20250310144959734](./paper%E7%AC%94%E8%AE%B0.assets/image-20250310144959734.png)
- 多头注意力机制：在单头注意力机制的基础上采用GPU并行计算，采用不同的$Q$、$K$、$V$矩阵去产生不同的注意力机制，关注不同的上下文关系。

// todo1 - 前置: CLIP, VIT

---

- [论文链接](https://arxiv.org/abs/2112.10752)
- 参考链接：[1](https://blog.csdn.net/weixin_57974242/article/details/134180461)

#### 1 模型解读

![image-20250310110335806](./paper%E7%AC%94%E8%AE%B0.assets/image-20250310110335806.png)

1. **输入图像（Pixel Space）**
      - **输入图像（x）**：这是你给模型的原始图片，或者是一张图片的一部分。比如，你想让模型根据这张图片生成一张新的图片。
      - **编码器（Ɛ）**：模型会把这张图片压缩成一个更小的、更抽象的表示形式，叫做“潜在表示”（z）。这个过程就像把一张高清图片压缩成一个小文件，方便后续处理。
2. **潜在空间（Latent Space）**
      - **潜在表示（z）**：这是编码器生成的压缩版图片信息，它包含了图片的关键特征，但去掉了不必要的细节。
      - **扩散过程（Diffusion Process）**：这是模型的核心部分。模型会在这个潜在表示上“加噪声”，也就是让图片信息变得模糊和混乱。然后，模型会学习如何一步步“去噪”，也就是把模糊的信息变回清晰的图片。
      - **去噪U-Net（εθ）**：这是一个神经网络，专门用来去除噪声。它通过多次迭代，逐步把模糊的潜在表示变回清晰的图片。这个网络里有一些特殊的机制：
            - **注意力机制（Q, K, V）**：帮助模型在处理图片时，专注于重要的部分。
            - **跨注意力（Cross-attention）**：让模型能够结合其他信息（比如文字或语义图）来生成图片。
            - **跳跃连接（Skip Connection）**：帮助信息在网络中更好地流动，避免丢失重要细节。
            - **拼接（Concat）**：把不同层次的信息结合起来，帮助模型更好地理解图片。
            - **开关（Switch）**：控制信息在网络中的流动方向。
3. **条件信息（Conditioning）**
      - **条件信息**：这是你给模型的额外信息，用来指导图片生成。比如：
            - **语义图（Semantic Map）**：告诉模型图片里应该有什么内容，比如“这里应该有一棵树”。
            - **文字（Text）**：用文字描述你想要的图片，比如“一只猫在草地上”。
            - **图片（Images）**：用另一张图片作为参考，生成类似的图片。
      - **条件处理模块（τθ）**：这个模块会把条件信息处理成适合模型使用的形式，然后把它融入到去噪U-Net中，帮助模型生成符合你要求的图片。
4. **输出图像（Pixel Space）**
   - **解码器（D）**：当模型在潜在空间中去噪完成后，解码器会把处理好的潜在表示（z）重新转换成一张完整的图片。
   - **输出图像（x̃）**：这就是模型最终生成的图片，它是根据你的输入和条件信息生成的。

##### 相比DDPM的最大改进：

1. 加入Autoencoder（上图中左侧红色部分），使得扩散过程在latent space下，提高图像生成的效率；
2. 加入条件机制，能够使用其他模态的数据控制图像的生成（上图中右侧灰色部分），其中条件生成控制通过Attention（上图中间部分QKV）机制实现。

#### 2 模型实现

##### 2.1 压缩学习阶段

大部分数字图像的比特位都对应着人眼无法察觉的细节。虽然 Diffusion Models (DMs) 能够通过最小化损失函数来抑制这些语义上无意义的信息，但是在训练和推理过程中，仍然需要评估所有像素，导致计算量过大。因此，文章提出使用 Latent Diffusion Models (LDMs) 作为一种有效的生成模型，并结合一个轻微的压缩阶段，来消除那些人眼无法察觉的细节，从而提高效率。

![image-20250310113152503](./paper%E7%AC%94%E8%AE%B0.assets/image-20250310113152503.png)

- **横轴（Rate, bits/dim）**：表示压缩的程度，数值越小，压缩得越厉害（文件越小）。
- **纵轴（Distortion, RMSE）**：表示图像失真程度，数值越大，图像质量越差。
- **感知压缩 (Perceptual Compression):** 这种压缩方式主要关注人眼能不能察觉到图像的变化。它会去除那些人眼不太敏感的细节，从而减小图像的大小。
- **语义压缩 (Semantic Compression):** 这种压缩方式更关注图像的“含义”或者“内容”。它会去除那些对图像理解没有太大影响的信息，即使这些信息在视觉上是可见的。

基于上述理论，我们在模型的压缩学习阶段利用一种自动编码模型，该模型学习一个在感知上与图像空间等效（在人的感知上和原始图像空间没什么区别），但计算复杂度显著降低的空间。

##### 2.2 生成学习阶段

就是传统的DM，加上部分条件信息。

##### 2.3 条件信息机制

1. 扩散模型和其他生成模型（如 GAN、VAE 等）一样，**理论上可以建模条件概率分布**$p(z|y)$。 
      - 这里的$y$是输入条件（如文本描述、语义分割图等），$z$是潜在空间的特征向量。
      - **目标**：在给定条件$y$的约束下，生成符合特定要求的样本。
2. **条件去噪自编码器**：通过设计一个条件去噪自编码器$\epsilon_{\theta}(z_t, t, y)$，扩散模型可以将条件信息$y$融入去噪过程。
      - $z_t$：时间步$t$时的含噪潜在向量
      - $t$：扩散过程的时间步（控制噪声强度）
      - $y$：额外的条件输入（如文本、语义图等）
      - 模型通过学习$\epsilon_{\theta}$，在去噪时同时考虑噪声$z_t$、时间步$t$和条件$y$，从而生成符合条件的潜在向量$z$
3. 通过不同输入控制生成过程**条件$y$的类型**决定**生成任务的方向**：
      - **文本引导的生成**：输入一段文本描述，生成符合描述的图像
      - **语义图到真实图像**：根据语义分割图（如 “天空”“草地” 等类别标签）生成对应的真实场景图像
      - **图像翻译任务**：将一种类型的图像转换为另一种类型（如将草图转换为照片、将 X 光片转换为 MRI 图像）

##### 2.4 交叉注意力机制

1. 引入**交叉注意力机制**增强UNet - 传统UNet结构擅长处理空间局部信息，但对全局依赖（如文本描述中的语义关联）建模能力有限。在引入交叉注意力机制后，能够：
      - **跨模态对齐**：将文本、图像等不同模态的信息进行注意力匹配。
      - **动态聚焦**：根据条件输入（如文本提示）选择性地关注图像生成的关键区域。
      例如当输入文本为 “一只红色的猫” 时，交叉注意力会引导模型在生成猫的区域时，特别关注 “红色” 对应的语义特征。
2. 引入**领域特定编码器**$\tau_{\theta}$，对齐不同模态的特征空间，将条件输入转换为中间表示，中间表示通过交叉注意力层与UNet的特征图进行交互，从而指导图像生成。
   ```c++
   输入条件 y（如文本） → τθ编码器 → 中间表示 τθ(y) → 交叉注意力层 → UNet生成潜在向量 z → 解码器 D → 输出图像
   ```

### 0-2-2 Taming Transformers for High-Resolution Image Synthesis

---

#### 前置1：VQVAE

学习链接：[1](https://spaces.ac.cn/archives/6760)

传统自回归模型在处理图像生成问题的时候是逐像素生成的：

![image-20250311124714888](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311124714888.png)

会存在以下问题：

1. 不同递归顺序会带来区别较大的结果
2. 慢，消耗算力大
3. 割裂类别间联系，可能肉眼无法区分的像素在计算loss的时候和区别很大的像素一样

而VAE通过最邻近将连续向量映射为了编码表中的一个固定向量，将其离散化：

![image-20250311125319634](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311125319634.png)

而为了保持重构时较小的失真率，我们将图片编码为$m×m$个$d$维向量：

![image-20250311125544612](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311125544612.png)

而这个离散的实现会带来两个问题，具体解决方式可以见原文：

1. 离散数据点带来的梯度消失问题，采用Straight-Through的思想在反向传播的时候**自行设计梯度**解决
      ![image-20250311130829879](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311130829879.png)
2. 为了保证我们的最邻近足够准确，将两个向量的距离也加入loss函数中并且调整两者占比参数进行优化，而且我们倾向于固定$z$让$z_q$去靠近$z$因为$z_q$是人为设计的可调的编码表，最终loss函数如下：
      ![image-20250311130809671](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311130809671.png)

---

- [论文链接](https://arxiv.org/abs/2012.09841)
- 参考链接：[1](https://blog.csdn.net/weixin_43357695/article/details/135995463) *其实写得一般，不如gpt(x*

#### 1 模型框架

模型的核心思想是结合了transformer和CNN的优点，使用CNN来有效地学习上下文丰富的视觉部分的codebook，然后Transformer学习它们的全局组合模型。

![image-20250310151736307](./paper%E7%AC%94%E8%AE%B0.assets/image-20250310151736307.png)

1. **VQGAN 预训练**：
      - CNN 编码器 **E** 将图像编码成潜在表示 **ẑ**。
      - 量化 **ẑ**，将其映射到离散的代码簿 **𝒵**，得到 **z_q**。
      - CNN 解码器 **G** 负责重建图像，判别器 **D** 进行对抗训练提高质量。
2. **Transformer 训练**：
      - 以 **z_q** 作为离散 token 序列，训练 Transformer 进行自回归建模。
3. **图像生成**：
      - 生成阶段，Transformer 根据已有 token 逐步预测 **z_q**。
      - 预测得到的 **z_q** 通过 VQGAN 解码器 **G** 生成最终的图像。
## 0-3 尝试在一个MLLM中统一生成和理解

以下两个模型的基本思路是一致的，都是把图片编码成离散的vision tokens。但是其实现的范式的不一样的，SEED基于diffusion model，而emu3直接基于VQGAN。

### 0-3-1 Making LLaMA SEE and Draw with SEED Tokenizer

- [论文链接](https://arxiv.org/abs/2310.01218)

#### 1 模型架构

![image-20250311152742653](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311152742653.png)

**(a) 视觉信号的 "翻译" 过程** - SEED会将提取特征后的2D图片转换成1D序列的Vision codes，这些vision code是有意义的语义单元（比如 "猫的尾巴"、"红色汽车"），而不是单纯的像素块

**(b) 多模态内容的 "编织" 过程** - 把Vision codes和Text混合成一条序列，使得多模态LLM可以使用next-word prediction执行可拓展的多模态自回归

而论文中为了保证理解和生成任务正常执行，生成的Vision code需要：

1. 有一维**因果依赖关系**的tokens - 因为原始图像是二维的，像素之间的依赖关系和传统文本处理下的单向注意力机制不匹配
2. 有**高层语义** - “把Vision codes和Text混合成一条序列”这个过程要求它们具有相同程度的语义

#### 2 训练方法

##### 2.1 SEED Tokenizer

![image-20250311164323893](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311164323893.png)

**核心目标**：把图像转换成适合大模型处理的 "视觉单词" 序列，同时保持语义连贯性和生成能力。

**主要组件与流程**：

1. **图像翻译器（ViT 编码器）** - 把图像切成 16x16 像素的小方块，并将每个方块转换成一个特征向量
2. **因果关系调整器（Causal Q-Former）** - 把原来 2D 网格排列的特征向量重新排列成 1D 序列并确保每个新标签生成时依赖前面所有标签的信息
3. **视觉字典（VQ Codebook）** - 把连续的特征向量转换成离散的视觉单词，每个视觉单词对应一个具体语义，保证生成的单词序列有因果关系（后面的词依赖前面的）
4. **生成控制器（MLP）** - 将视觉单词序列压缩成一个生成指令（1 个综合向量），这个指令能控制图像生成模型并且与预训练的图像生成模型（unCLIP-SD）的潜在空间对齐

**这个SD Decoder是基于diffusion的，所以这个模型的视觉token最终是用于生成扩散过程中的控制条件，也就是生成指令的。并不是next token prediction。**

**训练过程**：

1. 训练因果Q-Former - 是最小化图中"Contrastive"的loss的过程
2. 训练Tokenize和De-tokenize - 是最小化图中两个"Reconstruct"的loss的过程 - 训练两个空间的vision code、embedding对齐

##### 2.2 SEED-LLaMA

![image-20250311154746741](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311154746741.png)

**核心目标**：训练出可以在交错的视觉和文本数据上进行多模态自回归预测的LLM

**处理过程**：

1. 预处理：把文本和图像都变成token。
2. 输入LLM：把处理好的离散的tokens输入，用 “[IMG]” 标记表示图像的开始，模型会同时处理**图片信息**和**文字信息**。模型的任务是理解这些信息，并生成一段与图片和文字相关的描述。
3. 推理：生成输出的token序列，并且将这些token转换回可读的结果，图片符号被还原成图片，文字符号被还原成文字；最终输出是盛开的莲花图片和描述：“从花苞到盛开通常需要半个月”。

**训练过程**：

1. 多模态预训练 - 让模型理解 "图文混合语言" 的生成规律，使用next token prediction的方式让模型学会图文信息的相互依赖关系（如看到 "红色" 视觉词后更可能生成 "苹果" 文字）并且能像人类一样边想文字边想图像
2. 指令微调

### 0-3-2 Emu3: Next-Token Prediction is All You Need

- [论文链接](https://arxiv.org/abs/2409.18869)
- 参考链接：[1](https://blog.csdn.net/Together_CZ/article/details/143966869?ops_request_misc=%257B%2522request%255Fid%2522%253A%25226ce6597c971b982c13f201eb13cc3007%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=6ce6597c971b982c13f201eb13cc3007&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-143966869-null-null.142^v102^pc_search_result_base9&utm_term=Emu3%3A%20Next-Token%20Prediction%20is%20All%20You%20Need&spm=1018.2226.3001.4187)

#### 1 核心思路

通过将图像、文本和视频标记化为离散空间，我们在多模态序列的混合上从头开始训练一个单一的Transformer。

![image-20250311172034209](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311172034209.png)

#### 2 预训练

训练数据结构如下：

![image-20250311173220058](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311173220058.png)

用一个大Transformer模型处理所有模态的 token，使用标准的交叉熵损失进行下一个标记预测任务进行训练。为了防止视觉标记主导学习过程，我们对与视觉标记相关的损失应用0.5的权重。

预训练过程分为两个阶段。在第一阶段，不使用视频数据，从上下文长度为5120的文本和图像数据从头开始训练；在第二阶段，引入视频数据，并使用131072的上下文长度。

预测方式是next token prediction，基于VQGAN。

#### 3 后训练

- 任务定制微调：用特定任务数据集（如 COCO 图像描述数据集）训练
- 对比学习优化：让模型生成的图像与真实图像对比，调整参数使差异更小
    - //todo2：对比学习

- 指令对齐训练：输入多轮对话示例（如 "画一只戴眼镜的猫"），模型学习根据指令生成符合要求的输出
- 效率优化训练：在保持效果前提下，训练模型减少计算资源消耗

## 1-1 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation

- [论文链接](https://arxiv.org/abs/2410.13848)

- 参考链接：[1](https://blog.csdn.net/sexy19910923/article/details/143095754)
- [一个好用的翻译网站](https://doc2x.com/)

这个工作核心创新点是解耦视觉编码器，之前的工作在理解和生成的两个任务上采用了单一的视觉编码器，但是这两个任务所需的表征是存在显著差异的：

- **理解任务**更倾向于提取高级语义信息，是提取信息+推理的结合，因此重点在高维**语义表征**
- **生成任务**更倾向于局部细节与全局一致，重点在低维的**细节编码**

因此在这个工作我们引入了两条独立的视觉编码路径分别用于多模态的理解和生成，并且由相同的transformer架构统一起来。

### 1 模型架构

![image-20250311203559053](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311203559053.png)

- 理解任务
    - SigLIP编码器从图像中提取高维语义特征，从二维网格展平为一维序列并且映射到LLM输入空间
    - tokenize文本输入
    - 上述两种特征统一使用transformer处理
- 生成任务
    - 使用VQ tokenizer转换图像为离散特征ID并展平映射到输入空间
    - 拼接特征序列进行预测 - 内置预测头用于文本预测，使用随机初始化的预测头进行图像预测
    - 预测遵循自回归框架

### 2 训练过程

![image-20250311204057042](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311204057042.png)

**第一阶段：训练适配器和图像头。**该阶段的主要目标是在嵌入空间内创建视觉和语言元素之间的概念联系，使LLM能够理解图像中显示的实体并具有初步的视觉生成能力。在此阶段，保持视觉编码器和 LLM 冻结，只允许更新理解适配器、生成适配器和图像头中的可训练参数。

**第二阶段： 统一预训练。**在这个阶段，使用多模态语料库进行统一预训练，使 Janus 能够学习多模态理解和生成。解冻 LLM 并利用所有类型的训练数据：纯文本数据、多模态理解数据和视觉生成数据。受 Pixart 的启发，首先使用 ImageNet-1k 进行简单的视觉生成训练，以帮助模型掌握基本的像素依赖性。随后，使用通用文本到图像数据增强了模型的开放域视觉生成能力。

**第三阶段： 监督微调。**在此阶段，使用指令调整数据对预训练模型进行微调，以增强其指令跟随和对话能力。微调除生成编码器之外的所有参数。专注于监督答案，同时屏蔽系统和用户提示。为了确保 Janus 精通多模态理解和生成，不会针对特定任务微调单独的模型。相反，混合使用纯文本对话数据、多模态理解数据和视觉生成数据，确保跨各种场景的多功能性。

训练过程中采用的是交叉熵损失；推理过程中采用next-token prediction。

### 3 模型拓展

1. 多模态理解
      - 更强大的视觉编码器
      - 动态高分辨率技术
2. 视觉生成
      - 更细粒度编码器
      - 采用专门的视觉生成损失函数
      - 结合自回归（因果注意力）和并行（双向注意力）方法减少累积误差
3. 对额外模态的支持

## 1-2 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation

- [论文链接](https://arxiv.org/abs/2408.12528)
- 参考链接：[1](https://blog.csdn.net/buganything/article/details/141603951?ops_request_misc=%257B%2522request%255Fid%2522%253A%252209864a1e4f7a846855712b1153eb6fa0%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=09864a1e4f7a846855712b1153eb6fa0&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-6-141603951-null-null.142^v102^pc_search_result_base9&utm_term=show-o&spm=1018.2226.3001.4187)

在之前的多模态大模型中，普遍采用自回归预测方式，但是其因果注意力机制对于处理高分辨率的图像和视频任务需要大量的采样步骤。

本次工作尝试将单一的Transformer同时涉及自回归和diffusion modeling。不同于之前传统的连续扩散，我们采用离散去噪diffusion来对离散图像token进行建模、生成。

![image-20250311213412697](./paper%E7%AC%94%E8%AE%B0.assets/image-20250311213412697.png)

### 1 模型架构

继承了现有大语言模型的架构，除了在每个注意力层前添加一个QK-归一化的操作外没有对架构进行任何修改。

QK-Norm: 希望矩阵中的每一行都与其他行有相似的程度，避免出现少数几行与其他行非常相似，而大部分行与其他行都不太相似的情况。通过鼓励矩阵的均匀性，QK-Norm 可以帮助模型学习到更加鲁棒的特征表示，提高模型的泛化能力。

### 2 全注意力机制

![image-20250312135449007](./paper%E7%AC%94%E8%AE%B0.assets/image-20250312135449007.png)

全注意力机制：纵轴从上到下是token的前后顺序，每行的信息表示这个token可以注意的token

- 深色方块代表“允许参加”，而白色方块表示“阻止参加”。
- 在包含文本和图像标记的序列中，全向注意力机制使用**因果**注意处理**文本**标记，而使用**完全**注意处理**图像**标记。
- 关于输入序列：
    - 文本标记可以关注所有前面的图像标记；
    - 图像标记可以访问所有先前的文本标记；
    - 在仅提供文本标记的情况下，注意力会降级为因果注意力。

### 3 训练目标

![image-20250312140109313](./paper%E7%AC%94%E8%AE%B0.assets/image-20250312140109313.png)

总体训练损失最小化，而上述训练损失分为两部分：

1. mask token prediction预测损失$\mathcal{L}_{MTP}$ - 采用生成掩码模型的方法，随机掩盖图像标记，并训练模型预测原始值。从未掩码标记和前面的文本标记的掩码标记中重建原始图像标记。
      - 我的理解是这个训练了离散扩散模型，用于这个模型的视觉生成任务。
2. next token prediction预测损失$\mathcal{L}_{NTP}$ - 使用标准的语言表达模型目标，最大化文本标记的预测概率。
      - 我的理解是这个训练了自回归机制，用于这个模型的多模态理解任务。

### 4 训练过程

- **第一阶段**：图像标记嵌入和像素依赖性学习。使用RefinedWeb数据集进行语言模型训练，使用ImageNet-1K数据集进行类条件图像生成和图像描述训练。这个阶段的目标是：
    - **语言能力维持**
        - 数据集：RefinedWeb（海量文本）
        - 目标：保持文本理解与生成能力，避免语言模型退化
    - **图像生成能力**
        - 数据集：ImageNet-1K（类名作为文本输入）
        - 目标：学习离散图像 token 的可调整嵌入、像素依赖关系（如相邻像素构成物体），实现类条件图像生成（如输入 “猫” 生成对应图像）
    - **图文对齐能力**
        - 数据集：图像 - 文本对（如图像描述数据）
        - 目标：建立图像 token 与文本 token 的语义对应关系，支持图像描述任务（如输入图像生成准确文本）
- **第二阶段**：图像-文本对齐训练。主要关注图像描述和文本到图像生成的图像与文本对齐。
- **第三阶段**：高质量数据微调。使用过滤的高质量图像-文本对（用于文本到图像生成）和指令数据进行微调，以提高特定任务的性能。

### 5 推理

模型的任务分为两种类型：

#### **场景 1：视觉生成（文本→图像）**

- **输入**：条件文本 tokens + [MASK] tokens（例如：["红色", "汽车"] + `[MASK][MASK]`）。
- **输出**：通过离散扩散模型输出符合条件的图像 tokens（例如：["红色", "汽车", "轮胎", "火焰"]）。

#### **场景 2：多模态理解（图像→文本）**

- **输入**：图像 tokens + 条件文本（问题）（例如：图像 tokens + ["这只猫的特征是什么？"]）。
- **输出**：LLM通过自回归机制逐 token 生成符合问题的回答（例如："戴眼镜的猫"）。

当模型执行视觉生成任务的时候，输入为N 个条件文本 tokens + M 个占位符 [MASK]，模型分 T 步生成，类似逐帧绘制图像，每一步填充一个 [MASK]。每个[MASK]token的最终logit公式为：

![image-20250312135516873](./paper%E7%AC%94%E8%AE%B0.assets/image-20250312135516873.png)

- logit 计算：
    - 条件 logit 通过交叉注意力连接文本与视觉 tokens
    - 无条件 logit 来自模型的先验知识（如常见视觉模式）
- 公式作用：
    - $(1+w)ℓ_c^t$放大条件约束的影响
    - $-wℓ_u^t$削弱无条件预测的权重
    - 最终 logit 偏向符合输入文本的 tokens

**整体的工作原理就是：先进行文本token的next token prediction，在读到文本token的结束标记的时候切换为diffusion生成模型的工作状态，给几个图像切片利用之前所有预测的文本tokens作为控制条件输入模型进行T步去噪生成对应的vision tokens，再切换到next token prediction模式，而前面生成的文本token和vision token都作为模型的输入**

---

*附：一些在学习过程中的ai自编小故事（x 我当时觉得他说得挺对的后来发现和这篇论文不是一回事情*

**问题1：离散扩散到底是怎么与自回归机制融合的？**

1. **噪声注入阶段**
      - 在训练数据输入模型前，对图像或文本 token 序列施加高斯噪声或随机 mask。
      - 例如：将图像 token 序列随机替换为噪声 token，或在文本中随机删除部分单词。
2. **降噪对比损失（$L_{MTP}$）**
      - 模型需根据噪声污染的输入，预测原始的 clean token 序列。
      - 采用离散扩散模型的目标函数，计算预测 token 与原始 token 的交叉熵损失。
      - 作用：增强模型对噪声的鲁棒性，提升细节生成能力（如图像纹理、文本连贯性）。
3. **推理阶段**
      - 离散扩散仅用于训练阶段的正则化，推理时不参与实际生成。
      - 生成过程完全依赖自回归 Transformer，确保高效性（无需多步去噪）。

**问题2：模型训练机制**

“模型通过训练学会了在生成时自动处理噪声，而无需显式扩散步骤。”

1. **MTP Loss 的核心构成**
      - 关键组件：
          - **噪声注入**：对输入数据（文本 / 图像）添加高斯噪声或随机 mask（如文本删除 15% 的词，图像替换 30% 的像素块为噪声）。
          - **降噪目标**：模型需根据噪声数据$x_t$预测原始数据$x_0$，使用交叉熵（CE）计算损失。
          - **多步优化**：类似离散扩散模型，分阶段逐步去噪（如从强噪声到弱噪声），但总步数远少于传统扩散模型（如仅 5 步）。


2. **训练阶段优化的核心参数**
      - 共享参数：MTP 损失与自回归生成损失$L_{NTP}$共享同一套 Transformer 参数。
          - 例如：模型在训练时同时学习两个任务：
                1. **自回归生成**（如根据 “猫” 生成图像）。
                2. **降噪恢复**（如从模糊的 “猫” 图像恢复原图）。

      - 参数优化目标：
          - 通过梯度反向传播，使模型同时提升两个能力：
              - **生成能力**：通过$L_{NTP}$优化序列生成的连贯性。
              - **抗干扰能力**：通过$L_{MTP}$优化特征提取的鲁棒性（如从噪声中提取关键信息）。
3. **为什么生成阶段无需离散扩散？**
      - 参数学习的 “隐性去噪”：训练时，模型通过降噪任务（$L_{MTP}$）学会了从噪声中提取有效特征。例如：
          - 输入噪声图像 → 模型学习到 “猫的轮廓” 比噪声更重要 → 生成阶段即使输入无噪声，也能直接生成猫的清晰图像。
      - 架构设计：
          - 自回归 Transformer 的解码器已具备生成能力，无需额外去噪步骤。
          - 离散扩散仅在训练时作为 “老师”，教会模型如何处理噪声，但最终生成时 “学生”（自回归模块）已独立掌握能力。


**通俗示例：学习画猫**

- **训练阶段**：
    1. 给模型看正常猫的图片（$x_0$），要求生成对应的描述（$L_{NTP}$）。
    2. 给模型看模糊的猫图片（$x_t$），要求 “修复” 成原图（$L_{MTP}$）。
    - 模型学会：
        - 猫的关键特征（耳朵、胡须）比噪声更重要。
        - 如何根据残缺信息补全细节。

- **生成阶段**： 输入文字 “一只戴围巾的猫” → 模型直接生成清晰图像，无需逐步去噪。
    - 因为训练时已通过降噪任务学会：即使输入有噪声，也能提取 “围巾” 和 “猫” 的特征，生成时自然应用这些知识。

**总结**

- **MTP Loss**：通过噪声数据训练模型的抗干扰能力，优化共享参数。
- **生成阶段**：模型已将降噪能力 “固化” 到自回归模块中，无需显式扩散步骤。
- **核心思想**：用扩散训练增强模型鲁棒性，用自回归生成保证效率，实现 “学的时候复杂，用的时候简单”。

---