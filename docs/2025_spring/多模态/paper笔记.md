## 0-1-1 Visual Instruction Turing

- [论文链接](https://arxiv.org/abs/2304.08485)

- 参考链接：[1](https://blog.csdn.net/qq_58400270/article/details/135073408) [2](https://zhuanlan.zhihu.com/p/647782091)

### 1 前置知识

![image-20250303105247532](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303105247532.png)

指令微调和提示微调的目的都是去挖掘语言模型本身具备的知识。不同的是，Prompt 是激发语言模型的**补全能力**，例如根据上半句生成下半句，或是完形填空等。Instruct 是激发语言模型的**理解能力**，它通过给出更明显的指令，让模型去做出正确的行动。**指令微调的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示微调都是针对一个任务的。泛化能力不如指示学习**。

### 2 具体实现

#### 2.1 视觉指令数据集

用于指令微调的指示学习阶段。

![image-20250303111943784](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303111943784.png)

我们的输入并不是图片，而是描述图片的文本信息：captions和boxes。

训练过程中，模型通过图片描述和三类指令给出回答。

#### 2.2 LLaVa架构

![image-20250303111756590](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303111756590.png)

用一个简单的例子，比如你给系统输入一张狗的图片，并问它：“这是什么动物？”
LLaVA 处理这个问题的流程如下：

1. **图片输入 (`X_v`)**
      - 你上传一张狗的照片 (`X_v`)。
      - 视觉编码器（ViT-L/14）处理这张图片，提取重要特征，得到 `Z_v`（相当于“图片的数值版”）。
2. **特征转换 (`Z_v` → `H_v`)**
      - 由于 LLM 只能理解文本，所以需要把 `Z_v` 变成类似单词的格式。
      - 通过**投影矩阵 (`W`)**，把 `Z_v` 转换成**视觉标记 (`H_v`)**，让 LLM 能够理解。
3. **语言模型处理 (`H_v` + `H_q`)**
      - 你的问题 `"这是什么动物？"` (`X_q`) 也被语言模型处理。
      - 语言模型结合问题 (`H_q`) 和视觉信息 (`H_v`)，然后生成回答 (`X_a`)。
4. **输出 (`X_a`)**
      - 模型输出 `"这是一只狗"`，完成任务。

#### 2.3 训练过程

![image-20250303135208878](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135208878.png)

整体训练的输入输出模式为：

![image-20250303135334852](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135334852.png)

- 绿色的部分用于计算loss。*q: 为什么所有<STOP>token都要被计算？*

- 指令微调过程的输出计算采用自回归函数，也就是给定图像特征和指令后输出为a的条件概率为所有token在图像特征和之前所有token的条件下输出该token的条件概率相乘；对于长度为$L$的对话数据序列，计算可能的回答概率如下：
   ![image-20250303135721366](./paper%E7%AC%94%E8%AE%B0.assets/image-20250303135721366.png)

- $X_{\text{system-message}}$为系统要执行的任务：

  ```c++
  A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human’s questions. and <STOP> = ###
  ```

##### 2.3.1 第一阶段：特征对齐预训练

在2.2的架构中，我们看到语言模型会结合`H_v`和`H_q`给出回答，所以我们在这一步需要做的就是对齐两者的特征。

利用 [图像+人工标记的caption] 数据集，冻结视觉编码器和LLM的参数，仅训练线性层$\text{Projection}W$的参数，在构建输入$X_{instruction}$时，对于每一张图像$X_v$，随机抽取一个问题$X_q$ ，这个问题其实就是让模型（可以理解为助手）简单描述图像的语言指令。真实的预测答案$X_a$就是原来图像所配的文字描述（也就是原始的图像说明文字）。

而我们训练的过程就是调整线性层参数使得$X_a$的$p$最大化。

##### 2.3.2 第二阶段：端到端的微调

始终保持视觉编码器的权重冻结（即不更新视觉编码器的权重），继续更新投影层的预训练权重以及 LLaVA 中的大语言模型（LLM）的权重。

在这里我们考虑两个特定的用例场景：

1. chatbot: 在聊天机器人输出的三种类型回复中，对话类型是多轮的，而另外两种回复类型是单轮的。在训练过程中，对这三种回复类型进行均匀采样。
2. Science QA: 将数据组织成单轮对话形式，模型需要完成两项任务，一是以自然语言的形式提供推理过程，二是在多个选项中选择答案，训练中我们把问题和上下文作为$X_{instruction}$，把推理过程和答案作为$X_a$。

## 0-1-2 Taming Transformers for High-Resolution Image Synthesis

- [论文链接](https://arxiv.org/abs/2304.10592)
- 参考链接：[1](https://blog.csdn.net/weixin_43357695/article/details/135995463)

//todo
